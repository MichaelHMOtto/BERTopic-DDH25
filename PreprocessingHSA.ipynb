{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bc4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwordsiso in c:\\users\\michael\\appdata\\local\\anaconda3\\envs\\tei-mapper\\lib\\site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6ae16",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline HSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753376ee",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4c0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\michael\\AppData\\Local\\anaconda3\\envs\\tei-mapper\\Lib\\site-packages\\stopwordsiso\\_core.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import stopwordsiso as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "XML_NS = \"{http://www.w3.org/XML/1998/namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fac03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_paragraph_metadata_from_files(file_list):\n",
    "    all_rows = []\n",
    "\n",
    "    def get_id_and_surname(pn_nodes):\n",
    "        if not pn_nodes:\n",
    "            return None, None\n",
    "        pn = pn_nodes[0]\n",
    "        pid = pn.get(\"corresp\")\n",
    "        sname = pn.xpath(\"normalize-space(tei:surname)\", namespaces=TEI_NS) or None\n",
    "        return pid, sname\n",
    "\n",
    "    def text_of(elem):\n",
    "        raw = \"\".join(elem.itertext())\n",
    "        return \" \".join(raw.split())\n",
    "\n",
    "    def get_subject_pref_label(root):\n",
    "        label = root.xpath(\n",
    "            \"//tei:profileDesc/tei:textClass/tei:keywords[@scheme='https://gams.uni-graz.at/o:hsa.subjects']\"\n",
    "            \"/tei:term[@type='skos:Concept']/tei:term[@type='skos:prefLabel']/text()\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        return label[0].strip() if label else None\n",
    "\n",
    "    for file_path in file_list:\n",
    "        tree = etree.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        sender_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        recv_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='received']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "\n",
    "        sender_id, sender_surname = get_id_and_surname(sender_pn)\n",
    "        receiver_id, receiver_surname = get_id_and_surname(recv_pn)\n",
    "        lang = root.xpath(\"//tei:body/tei:div[@subtype='original']/@xml:lang\", namespaces=TEI_NS)\n",
    "        lang = lang[0] if lang else None\n",
    "\n",
    "        subject_label = get_subject_pref_label(root)  \n",
    "\n",
    "        date_sent = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:date/@when\",\n",
    "            namespaces=TEI_NS\n",
    "        )\n",
    "        date_sent = date_sent[0] if date_sent else None\n",
    "\n",
    "        # iterate over letters\n",
    "        letter_divs = root.xpath(\"//tei:text/tei:body//tei:div[@type='letter']\", namespaces=TEI_NS)\n",
    "        for div in letter_divs:\n",
    "            xml_id = div.get(f\"{XML_NS}id\", \"\")\n",
    "            if xml_id.startswith(\"L.\") and \".\" in xml_id:\n",
    "                letter_num = xml_id.split(\".\", 1)[1]\n",
    "            else:\n",
    "                letter_num = xml_id or \"1\"\n",
    "\n",
    "            paragraphs = div.xpath(\".//tei:p\", namespaces=TEI_NS)\n",
    "            for idx, p in enumerate(paragraphs, start=1):\n",
    "                pid = f\"L.{letter_num}-{idx}\"\n",
    "                all_rows.append({\n",
    "                    \"source_file\": os.path.basename(file_path),\n",
    "                    \"pid\": pid,\n",
    "                    \"sender_id\": sender_id,\n",
    "                    \"sender\": sender_surname,\n",
    "                    \"receiver_id\": receiver_id,\n",
    "                    \"receiver\": receiver_surname,\n",
    "                    \"date\": date_sent,\n",
    "                    \"text\": text_of(p),\n",
    "                    \"language\": lang,\n",
    "                    \"keywords\": subject_label,  \n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"source_file\", \"pid\",\n",
    "        \"sender_id\", \"sender\",\n",
    "        \"receiver_id\", \"receiver\", \"date\",\n",
    "        \"text\", \"language\", \"keywords\"\n",
    "    ])\n",
    "    return all_rows, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dd9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(text.strip().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e91dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_paragraphs(df, threshold=256):\n",
    "\n",
    "    if \"word_count\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'word_count' column.\")\n",
    "\n",
    "    long_df = df[df[\"word_count\"] > threshold].copy()\n",
    "    count = len(long_df)\n",
    "    return long_df, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91c82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_paragraphs(long_df: pd.DataFrame, chunk_size: int = 511):\n",
    "\n",
    "    required_cols = {\"pid\", \"text\"}\n",
    "    missing = required_cols - set(long_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s): {', '.join(sorted(missing))}\")\n",
    "\n",
    "    all_sections = []\n",
    "    meta_cols = [c for c in long_df.columns if c not in (\"pid\", \"text\")]\n",
    "\n",
    "    for _, row in long_df.iterrows():\n",
    "        parent_pid = row[\"pid\"]\n",
    "        text = row[\"text\"] or \"\"\n",
    "        words = text.strip().split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        section_num = 0\n",
    "        for start in range(0, len(words), chunk_size):\n",
    "            section_num += 1\n",
    "            chunk_words = words[start:start + chunk_size]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "            section_id = f\"{parent_pid}-{section_num}\"  # e.g., L.1-2-1\n",
    "\n",
    "            section_row = {\n",
    "                \"section_id\": section_id,\n",
    "                \"parent_pid\": parent_pid,\n",
    "                \"section_number\": section_num,\n",
    "                \"section_text\": chunk_text,\n",
    "                \"section_word_count\": len(chunk_words),\n",
    "            }\n",
    "\n",
    "            for c in meta_cols:\n",
    "                section_row[c] = row[c]\n",
    "\n",
    "            all_sections.append(section_row)\n",
    "\n",
    "    df_sections = pd.DataFrame(all_sections, columns=[\n",
    "        \"section_id\", \"parent_pid\", \"section_number\",\n",
    "        \"section_text\", \"section_word_count\", *meta_cols\n",
    "    ])\n",
    "    return all_sections, df_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511, pad_sections=False):\n",
    "\n",
    "    # sanity \n",
    "    if not df.columns.is_unique:\n",
    "        dups = df.columns[df.columns.duplicated()].tolist()\n",
    "        raise ValueError(f\"Input df has duplicate column names: {dups}\")\n",
    "\n",
    "    required_cols = {\"pid\", \"text\", \"word_count\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s) in df: {', '.join(sorted(missing))}\")\n",
    "\n",
    "\n",
    "    short_df = df[df[\"word_count\"] <= threshold].copy()\n",
    "    long_df  = df[df[\"word_count\"] >  threshold].copy()\n",
    "\n",
    "\n",
    "    _, sections_df = split_long_paragraphs(long_df, chunk_size=chunk_size)\n",
    "\n",
    "    if pad_sections:\n",
    "        # zum Beispiel L.x-y-01 \n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str).str.zfill(2)\n",
    "    else:\n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str)\n",
    "\n",
    "    sections_df[\"text\"] = sections_df[\"section_text\"]\n",
    "    sections_df[\"word_count\"]     = sections_df[\"section_word_count\"]\n",
    "\n",
    "    base_cols = list(df.columns)\n",
    "\n",
    "    # add cols\n",
    "    for c in base_cols:\n",
    "        if c not in sections_df.columns:\n",
    "            sections_df[c] = pd.NA\n",
    "\n",
    "    helper_cols = [c for c in sections_df.columns if c not in base_cols]\n",
    "    if helper_cols:\n",
    "        sections_df = sections_df.drop(columns=helper_cols)\n",
    "\n",
    "\n",
    "    sections_df = sections_df[base_cols]\n",
    "\n",
    "    # concat back \n",
    "    combined_df = pd.concat([short_df, sections_df], ignore_index=True, sort=False)\n",
    "    sort_cols = [c for c in [\"source_file\", \"paragraph_id\"] if c in combined_df.columns]\n",
    "    if sort_cols:\n",
    "        combined_df = combined_df.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ee8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_para_per_amount_of_words(df, amount = 511):\n",
    "    mask = df[\"text\"].apply(lambda text: len(str(text).split()) > amount)\n",
    "    return df.loc[mask, [\"pid\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ab3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_languages(df):\n",
    "    return sorted(lang for lang in df[\"language\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5674f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         source_file     pid                                      sender_id  \\\n",
      "0   hsa.letter.1.xml   L.1-1  https://gams.uni-graz.at/o:hsa.persons#P.1069   \n",
      "1   hsa.letter.1.xml   L.1-2  https://gams.uni-graz.at/o:hsa.persons#P.1069   \n",
      "2  hsa.letter.10.xml  L.10-1  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "3  hsa.letter.10.xml  L.10-2  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "4  hsa.letter.10.xml  L.10-3  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "\n",
      "              sender                                   receiver_id  \\\n",
      "0            Baissac  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "1            Baissac  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "2  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "3  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "4  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "\n",
      "     receiver        date                                               text  \\\n",
      "0  Schuchardt  1885-01-20  Ma Doudou vous envoie une petite brochure jaun...   \n",
      "1  Schuchardt  1885-01-20  Nous sommes anxieux l’un et l’autre d’avoir de...   \n",
      "2  Schuchardt        1882  contesto a todas sus anteriores en forma teleg...   \n",
      "3  Schuchardt        1882  1º Sanjurjo ha sido nombrado catedrático de Ma...   \n",
      "4  Schuchardt        1882  2º No he podido averiguar el nombre del gobern...   \n",
      "\n",
      "  language                       keywords  word_count  \n",
      "0       fr                           None          28  \n",
      "1       fr                           None          31  \n",
      "2       es  Giornale di Filologia Romanza           8  \n",
      "3       es  Giornale di Filologia Romanza          78  \n",
      "4       es  Giornale di Filologia Romanza          35  \n"
     ]
    }
   ],
   "source": [
    "folder_path = \"get\"  # <- adjust if needed\n",
    "file_pattern = \"*.xml\"  \n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "rows, df = extract_paragraph_metadata_from_files(file_list)\n",
    "df[\"word_count\"] = df[\"text\"].apply(count_words)\n",
    "\n",
    "df_final = replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "336aafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'ca', 'cy', 'da', 'de', 'en', 'es', 'eu', 'fr', 'ft', 'hu', 'idb', 'io', 'it', 'la', 'lad', 'ms', 'nl', 'pap', 'pt', 'ro', 'roa']\n",
      "Empty DataFrame\n",
      "Columns: [pid, text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "languages = get_unique_languages(df)\n",
    "print(languages)\n",
    "\n",
    "df_511 = get_para_per_amount_of_words(df_final, amount = 511)\n",
    "print(df_511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f248c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver_id</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>keywords</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L.1-1</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1069</td>\n",
       "      <td>Baissac</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1885-01-20</td>\n",
       "      <td>Ma Doudou vous envoie une petite brochure jaun...</td>\n",
       "      <td>fr</td>\n",
       "      <td>None</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L.1-2</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1069</td>\n",
       "      <td>Baissac</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1885-01-20</td>\n",
       "      <td>Nous sommes anxieux l’un et l’autre d’avoir de...</td>\n",
       "      <td>fr</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L.10-1</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.2100</td>\n",
       "      <td>Machado y Álvarez</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1882</td>\n",
       "      <td>contesto a todas sus anteriores en forma teleg...</td>\n",
       "      <td>es</td>\n",
       "      <td>Giornale di Filologia Romanza</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L.10-2</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.2100</td>\n",
       "      <td>Machado y Álvarez</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1882</td>\n",
       "      <td>1º Sanjurjo ha sido nombrado catedrático de Ma...</td>\n",
       "      <td>es</td>\n",
       "      <td>Giornale di Filologia Romanza</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L.10-3</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.2100</td>\n",
       "      <td>Machado y Álvarez</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1882</td>\n",
       "      <td>2º No he podido averiguar el nombre del gobern...</td>\n",
       "      <td>es</td>\n",
       "      <td>Giornale di Filologia Romanza</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid                                      sender_id             sender  \\\n",
       "0   L.1-1  https://gams.uni-graz.at/o:hsa.persons#P.1069            Baissac   \n",
       "1   L.1-2  https://gams.uni-graz.at/o:hsa.persons#P.1069            Baissac   \n",
       "2  L.10-1  https://gams.uni-graz.at/o:hsa.persons#P.2100  Machado y Álvarez   \n",
       "3  L.10-2  https://gams.uni-graz.at/o:hsa.persons#P.2100  Machado y Álvarez   \n",
       "4  L.10-3  https://gams.uni-graz.at/o:hsa.persons#P.2100  Machado y Álvarez   \n",
       "\n",
       "                                    receiver_id    receiver        date  \\\n",
       "0  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1885-01-20   \n",
       "1  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1885-01-20   \n",
       "2  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt        1882   \n",
       "3  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt        1882   \n",
       "4  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt        1882   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Ma Doudou vous envoie une petite brochure jaun...       fr   \n",
       "1  Nous sommes anxieux l’un et l’autre d’avoir de...       fr   \n",
       "2  contesto a todas sus anteriores en forma teleg...       es   \n",
       "3  1º Sanjurjo ha sido nombrado catedrático de Ma...       es   \n",
       "4  2º No he podido averiguar el nombre del gobern...       es   \n",
       "\n",
       "                        keywords  word_count  \n",
       "0                           None          28  \n",
       "1                           None          31  \n",
       "2  Giornale di Filologia Romanza           8  \n",
       "3  Giornale di Filologia Romanza          78  \n",
       "4  Giornale di Filologia Romanza          35  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = {\"source_file\"}\n",
    "df_final = df_final.drop(columns=drop)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335c3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"letters_py.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd03d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr True\n",
      "es True\n",
      "de True\n",
      "it True\n",
      "en True\n",
      "hu True\n",
      "pt True\n",
      "ar True\n",
      "cy False\n",
      "lad False\n",
      "eu True\n",
      "ft False\n",
      "nl True\n",
      "ms True\n",
      "da True\n",
      "roa False\n",
      "ca True\n",
      "la True\n",
      "None False\n",
      "ro True\n",
      "idb False\n",
      "pap False\n",
      "io False\n"
     ]
    }
   ],
   "source": [
    "langs = df_final['language'].unique()\n",
    "for lang in langs:\n",
    "    print(lang, stopwords.has_lang(lang))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tei-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
