{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6ae16",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline HSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753376ee",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import stopwordsiso as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "XML_NS = \"{http://www.w3.org/XML/1998/namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fac03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_paragraph_metadata_from_files(file_list):\n",
    "    all_rows = []\n",
    "\n",
    "    def get_id_and_surname(pn_nodes):\n",
    "        if not pn_nodes:\n",
    "            return None, None\n",
    "        pn = pn_nodes[0]\n",
    "        pid = pn.get(\"corresp\")\n",
    "        sname = pn.xpath(\"normalize-space(tei:surname)\", namespaces=TEI_NS) or None\n",
    "        return pid, sname\n",
    "\n",
    "    def text_of(elem):\n",
    "        raw = \"\".join(elem.itertext())\n",
    "        return \" \".join(raw.split())\n",
    "\n",
    "    def get_subject_pref_label(root):\n",
    "        label = root.xpath(\n",
    "            \"//tei:profileDesc/tei:textClass/tei:keywords[@scheme='https://gams.uni-graz.at/o:hsa.subjects']\"\n",
    "            \"/tei:term[@type='skos:Concept']/tei:term[@type='skos:prefLabel']/text()\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        return label[0].strip() if label else None\n",
    "\n",
    "    for file_path in file_list:\n",
    "        tree = etree.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        sender_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        recv_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='received']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "\n",
    "        sender_id, sender_surname = get_id_and_surname(sender_pn)\n",
    "        receiver_id, receiver_surname = get_id_and_surname(recv_pn)\n",
    "        lang = root.xpath(\"//tei:body/tei:div[@subtype='original']/@xml:lang\", namespaces=TEI_NS)\n",
    "        lang = lang[0] if lang else None\n",
    "\n",
    "        subject_label = get_subject_pref_label(root)  \n",
    "\n",
    "        date_sent = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:date/@when\",\n",
    "            namespaces=TEI_NS\n",
    "        )\n",
    "        date_sent = date_sent[0] if date_sent else None\n",
    "\n",
    "        # iterate over letters\n",
    "        letter_divs = root.xpath(\"//tei:text/tei:body//tei:div[@type='letter']\", namespaces=TEI_NS)\n",
    "        for div in letter_divs:\n",
    "            xml_id = div.get(f\"{XML_NS}id\", \"\")\n",
    "            if xml_id.startswith(\"L.\") and \".\" in xml_id:\n",
    "                letter_num = xml_id.split(\".\", 1)[1]\n",
    "            else:\n",
    "                letter_num = xml_id or \"1\"\n",
    "\n",
    "            paragraphs = div.xpath(\".//tei:p\", namespaces=TEI_NS)\n",
    "            for idx, p in enumerate(paragraphs, start=1):\n",
    "                pid = f\"L.{letter_num}-{idx}\"\n",
    "                all_rows.append({\n",
    "                    \"source_file\": os.path.basename(file_path),\n",
    "                    \"pid\": pid,\n",
    "                    \"sender_id\": sender_id,\n",
    "                    \"sender\": sender_surname,\n",
    "                    \"receiver_id\": receiver_id,\n",
    "                    \"receiver\": receiver_surname,\n",
    "                    \"date\": date_sent,\n",
    "                    \"text\": text_of(p),\n",
    "                    \"language\": lang,\n",
    "                    \"keywords\": subject_label,  \n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"source_file\", \"pid\",\n",
    "        \"sender_id\", \"sender\",\n",
    "        \"receiver_id\", \"receiver\", \"date\",\n",
    "        \"text\", \"language\", \"keywords\"\n",
    "    ])\n",
    "    return all_rows, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(text.strip().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e91dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_paragraphs(df, threshold=256):\n",
    "\n",
    "    if \"word_count\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'word_count' column.\")\n",
    "\n",
    "    long_df = df[df[\"word_count\"] > threshold].copy()\n",
    "    count = len(long_df)\n",
    "    return long_df, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_paragraphs(long_df: pd.DataFrame, chunk_size: int = 511):\n",
    "\n",
    "    required_cols = {\"pid\", \"text\"}\n",
    "    missing = required_cols - set(long_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s): {', '.join(sorted(missing))}\")\n",
    "\n",
    "    all_sections = []\n",
    "    meta_cols = [c for c in long_df.columns if c not in (\"pid\", \"text\")]\n",
    "\n",
    "    for _, row in long_df.iterrows():\n",
    "        parent_pid = row[\"pid\"]\n",
    "        text = row[\"text\"] or \"\"\n",
    "        words = text.strip().split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        section_num = 0\n",
    "        for start in range(0, len(words), chunk_size):\n",
    "            section_num += 1\n",
    "            chunk_words = words[start:start + chunk_size]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "            section_id = f\"{parent_pid}-{section_num}\"  # e.g., L.1-2-1\n",
    "\n",
    "            section_row = {\n",
    "                \"section_id\": section_id,\n",
    "                \"parent_pid\": parent_pid,\n",
    "                \"section_number\": section_num,\n",
    "                \"section_text\": chunk_text,\n",
    "                \"section_word_count\": len(chunk_words),\n",
    "            }\n",
    "\n",
    "            for c in meta_cols:\n",
    "                section_row[c] = row[c]\n",
    "\n",
    "            all_sections.append(section_row)\n",
    "\n",
    "    df_sections = pd.DataFrame(all_sections, columns=[\n",
    "        \"section_id\", \"parent_pid\", \"section_number\",\n",
    "        \"section_text\", \"section_word_count\", *meta_cols\n",
    "    ])\n",
    "    return all_sections, df_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511, pad_sections=False):\n",
    "\n",
    "    # sanity \n",
    "    if not df.columns.is_unique:\n",
    "        dups = df.columns[df.columns.duplicated()].tolist()\n",
    "        raise ValueError(f\"Input df has duplicate column names: {dups}\")\n",
    "\n",
    "    required_cols = {\"pid\", \"text\", \"word_count\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s) in df: {', '.join(sorted(missing))}\")\n",
    "\n",
    "\n",
    "    short_df = df[df[\"word_count\"] <= threshold].copy()\n",
    "    long_df  = df[df[\"word_count\"] >  threshold].copy()\n",
    "\n",
    "\n",
    "    _, sections_df = split_long_paragraphs(long_df, chunk_size=chunk_size)\n",
    "\n",
    "    if pad_sections:\n",
    "        # zum Beispiel L.x-y-01 \n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str).str.zfill(2)\n",
    "    else:\n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str)\n",
    "\n",
    "    sections_df[\"text\"] = sections_df[\"section_text\"]\n",
    "    sections_df[\"word_count\"]     = sections_df[\"section_word_count\"]\n",
    "\n",
    "    base_cols = list(df.columns)\n",
    "\n",
    "    # add cols\n",
    "    for c in base_cols:\n",
    "        if c not in sections_df.columns:\n",
    "            sections_df[c] = pd.NA\n",
    "\n",
    "    helper_cols = [c for c in sections_df.columns if c not in base_cols]\n",
    "    if helper_cols:\n",
    "        sections_df = sections_df.drop(columns=helper_cols)\n",
    "\n",
    "\n",
    "    sections_df = sections_df[base_cols]\n",
    "\n",
    "    # concat back \n",
    "    combined_df = pd.concat([short_df, sections_df], ignore_index=True, sort=False)\n",
    "    sort_cols = [c for c in [\"source_file\", \"paragraph_id\"] if c in combined_df.columns]\n",
    "    if sort_cols:\n",
    "        combined_df = combined_df.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_para_per_amount_of_words(df, amount = 511):\n",
    "    mask = df[\"text\"].apply(lambda text: len(str(text).split()) > amount)\n",
    "    return df.loc[mask, [\"pid\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_languages(df):\n",
    "    return sorted(lang for lang in df[\"language\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"get\"  # <- adjust if needed\n",
    "file_pattern = \"*.xml\"  \n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "rows, df = extract_paragraph_metadata_from_files(file_list)\n",
    "df[\"word_count\"] = df[\"text\"].apply(count_words)\n",
    "\n",
    "df_final = replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336aafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = get_unique_languages(df)\n",
    "print(languages)\n",
    "\n",
    "df_511 = get_para_per_amount_of_words(df_final, amount = 511)\n",
    "print(df_511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = {\"source_file\"}\n",
    "df_final = df_final.drop(columns=drop)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"letters_py.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd03d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = df_final['language'].unique()\n",
    "for lang in langs:\n",
    "    print(lang, stopwords.has_lang(lang))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tei-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
