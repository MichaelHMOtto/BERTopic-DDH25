{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bc4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwordsiso in c:\\users\\michael\\appdata\\local\\anaconda3\\envs\\tei-mapper\\lib\\site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6ae16",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline HSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753376ee",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4c0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\michael\\AppData\\Local\\anaconda3\\envs\\tei-mapper\\Lib\\site-packages\\stopwordsiso\\_core.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import stopwordsiso as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "XML_NS = \"{http://www.w3.org/XML/1998/namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fac03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraph_metadata_from_files(file_list):\n",
    "    all_rows = []\n",
    "\n",
    "    def get_id_and_surname(pn_nodes):\n",
    "        if not pn_nodes:\n",
    "            return None, None\n",
    "        pn = pn_nodes[0]\n",
    "        pid = pn.get(\"corresp\")\n",
    "        sname = pn.xpath(\"normalize-space(tei:surname)\", namespaces=TEI_NS) or None\n",
    "        return pid, sname\n",
    "\n",
    "    def text_of(elem):\n",
    "        # take all text nodes not inside <tei:note>, then collapse whitespace\n",
    "        parts = elem.xpath(\".//text()[not(ancestor::tei:note)]\", namespaces=TEI_NS)\n",
    "        return \" \".join(\"\".join(parts).split())\n",
    "\n",
    "    def get_subject_pref_label(root):\n",
    "        label = root.xpath(\n",
    "            \"//tei:profileDesc/tei:textClass/tei:keywords[@scheme='https://gams.uni-graz.at/o:hsa.subjects']\"\n",
    "            \"/tei:term[@type='skos:Concept']/tei:term[@type='skos:prefLabel']/text()\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        return label[0].strip() if label else None\n",
    "\n",
    "    for file_path in file_list:\n",
    "        tree = etree.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        sender_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        recv_pn = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='received']/tei:persName\",\n",
    "            namespaces=TEI_NS,\n",
    "        )\n",
    "        sender_id, sender_surname = get_id_and_surname(sender_pn)\n",
    "        receiver_id, receiver_surname = get_id_and_surname(recv_pn)\n",
    "\n",
    "        # Prefer language of ORIGINAL block\n",
    "        lang = root.xpath(\"//tei:body/tei:div[@type='letter' and @subtype='original']/@xml:lang\", namespaces=TEI_NS)\n",
    "        if not lang:\n",
    "            lang = root.xpath(\"//tei:body/tei:div[@subtype='original']/@xml:lang\", namespaces=TEI_NS)\n",
    "        lang = lang[0] if lang else None\n",
    "\n",
    "        subject_label = get_subject_pref_label(root)\n",
    "\n",
    "        date_sent = root.xpath(\n",
    "            \"//tei:teiHeader//tei:correspDesc/tei:correspAction[@type='sent']/tei:date/@when\",\n",
    "            namespaces=TEI_NS\n",
    "        )\n",
    "        date_sent = date_sent[0] if date_sent else None\n",
    "\n",
    "        # Collect ONLY original letters \n",
    "        original_divs = root.xpath(\n",
    "            \"//tei:text/tei:body//tei:div[@type='letter' and @subtype='original']\",\n",
    "            namespaces=TEI_NS\n",
    "        )\n",
    "\n",
    "        if not original_divs:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        def pick_base_letter_num(root, divs, file_path):\n",
    "            #  Canonical PID:  \"o:hsa.letter.6060\"\n",
    "            pid_vals = root.xpath(\n",
    "                \"//tei:fileDesc/tei:publicationStmt/tei:idno[@type='PID']/text()\",\n",
    "                namespaces=TEI_NS\n",
    "            )\n",
    "            if pid_vals:\n",
    "                pid_text = pid_vals[0].strip()\n",
    "                m = re.search(r'letter\\.([A-Za-z0-9_-]+)$', pid_text)\n",
    "                if m:\n",
    "                    return m.group(1)       # -> \"L.6060\"\n",
    "                return pid_text\n",
    "\n",
    "            # Fallback to correspDesc @ref: \"...o:hsa.letter.6060\"\n",
    "            ref_vals = root.xpath(\"//tei:profileDesc/tei:correspDesc/@ref\", namespaces=TEI_NS)\n",
    "            if ref_vals:\n",
    "                m = re.search(r'letter\\.([A-Za-z0-9_-]+)$', ref_vals[0])\n",
    "                if m:\n",
    "                    return m.group(1)\n",
    "\n",
    "\n",
    "            for d in divs:\n",
    "                xid = d.get(f\"{XML_NS}id\", \"\")\n",
    "                if xid and xid.startswith(\"L.\") and \".\" in xid:\n",
    "                    return xid\n",
    "\n",
    "            # deterministic fallback\n",
    "            base = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            return f\"fallback_{base}\"\n",
    "\n",
    "        letter_base = pick_base_letter_num(root, original_divs, file_path)\n",
    "\n",
    "        \n",
    "        para_idx = 1\n",
    "        for div in original_divs:\n",
    "            paragraphs = div.xpath(\".//tei:p\", namespaces=TEI_NS)\n",
    "            for p in paragraphs:\n",
    "                txt = text_of(p).strip()\n",
    "                if len(txt) < 1:   # skip empty <p/> elements, adjustable to skip shorter paragraphs\n",
    "                    continue\n",
    "\n",
    "                pid = f\"L.{letter_base}-{para_idx}\"\n",
    "                para_idx += 1\n",
    "\n",
    "                all_rows.append({\n",
    "                    \"source_file\": os.path.basename(file_path),\n",
    "                    \"pid\": pid,\n",
    "                    \"sender_id\": sender_id,\n",
    "                    \"sender\": sender_surname,\n",
    "                    \"receiver_id\": receiver_id,\n",
    "                    \"receiver\": receiver_surname,\n",
    "                    \"date\": date_sent,\n",
    "                    \"text\": txt,\n",
    "                    \"language\": lang,\n",
    "                    \"keywords\": subject_label,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"source_file\", \"pid\",\n",
    "        \"sender_id\", \"sender\",\n",
    "        \"receiver_id\", \"receiver\", \"date\",\n",
    "        \"text\", \"language\", \"keywords\"\n",
    "    ])\n",
    "    return all_rows, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dd9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(text.strip().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e91dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_paragraphs(df, threshold=256):\n",
    "\n",
    "    if \"word_count\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'word_count' column.\")\n",
    "\n",
    "    long_df = df[df[\"word_count\"] > threshold].copy()\n",
    "    count = len(long_df)\n",
    "    return long_df, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91c82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_paragraphs(long_df: pd.DataFrame, chunk_size: int = 511):\n",
    "\n",
    "    required_cols = {\"pid\", \"text\"}\n",
    "    missing = required_cols - set(long_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s): {', '.join(sorted(missing))}\")\n",
    "\n",
    "    all_sections = []\n",
    "    meta_cols = [c for c in long_df.columns if c not in (\"pid\", \"text\")]\n",
    "\n",
    "    for _, row in long_df.iterrows():\n",
    "        parent_pid = row[\"pid\"]\n",
    "        text = row[\"text\"] or \"\"\n",
    "        words = text.strip().split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        section_num = 0\n",
    "        for start in range(0, len(words), chunk_size):\n",
    "            section_num += 1\n",
    "            chunk_words = words[start:start + chunk_size]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "            section_id = f\"{parent_pid}-{section_num}\"  # e.g., L.1-2-1\n",
    "\n",
    "            section_row = {\n",
    "                \"section_id\": section_id,\n",
    "                \"parent_pid\": parent_pid,\n",
    "                \"section_number\": section_num,\n",
    "                \"section_text\": chunk_text,\n",
    "                \"section_word_count\": len(chunk_words),\n",
    "            }\n",
    "\n",
    "            for c in meta_cols:\n",
    "                section_row[c] = row[c]\n",
    "\n",
    "            all_sections.append(section_row)\n",
    "\n",
    "    df_sections = pd.DataFrame(all_sections, columns=[\n",
    "        \"section_id\", \"parent_pid\", \"section_number\",\n",
    "        \"section_text\", \"section_word_count\", *meta_cols\n",
    "    ])\n",
    "    return all_sections, df_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511, pad_sections=False):\n",
    "\n",
    "    # sanity \n",
    "    if not df.columns.is_unique:\n",
    "        dups = df.columns[df.columns.duplicated()].tolist()\n",
    "        raise ValueError(f\"Input df has duplicate column names: {dups}\")\n",
    "\n",
    "    required_cols = {\"pid\", \"text\", \"word_count\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s) in df: {', '.join(sorted(missing))}\")\n",
    "\n",
    "\n",
    "    short_df = df[df[\"word_count\"] <= threshold].copy()\n",
    "    long_df  = df[df[\"word_count\"] >  threshold].copy()\n",
    "\n",
    "\n",
    "    _, sections_df = split_long_paragraphs(long_df, chunk_size=chunk_size)\n",
    "\n",
    "    if pad_sections:\n",
    "        # zum Beispiel L.x-y-01 \n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str).str.zfill(2)\n",
    "    else:\n",
    "        sections_df[\"pid\"] = sections_df[\"parent_pid\"] + \"-\" + sections_df[\"section_number\"].astype(str)\n",
    "\n",
    "    sections_df[\"text\"] = sections_df[\"section_text\"]\n",
    "    sections_df[\"word_count\"]     = sections_df[\"section_word_count\"]\n",
    "\n",
    "    base_cols = list(df.columns)\n",
    "\n",
    "    # add cols\n",
    "    for c in base_cols:\n",
    "        if c not in sections_df.columns:\n",
    "            sections_df[c] = pd.NA\n",
    "\n",
    "    helper_cols = [c for c in sections_df.columns if c not in base_cols]\n",
    "    if helper_cols:\n",
    "        sections_df = sections_df.drop(columns=helper_cols)\n",
    "\n",
    "\n",
    "    sections_df = sections_df[base_cols]\n",
    "\n",
    "    # concat back \n",
    "    combined_df = pd.concat([short_df, sections_df], ignore_index=True, sort=False)\n",
    "    sort_cols = [c for c in [\"source_file\", \"paragraph_id\"] if c in combined_df.columns]\n",
    "    if sort_cols:\n",
    "        combined_df = combined_df.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ee8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_para_per_amount_of_words(df, amount = 511):\n",
    "    mask = df[\"text\"].apply(lambda text: len(str(text).split()) > amount)\n",
    "    return df.loc[mask, [\"pid\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ab3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_languages(df):\n",
    "    return sorted(lang for lang in df[\"language\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"get\"  # <- adjust if needed\n",
    "file_pattern = \"*.xml\"  \n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dddb0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         source_file     pid                                      sender_id  \\\n",
      "0   hsa.letter.1.xml   L.1-1  https://gams.uni-graz.at/o:hsa.persons#P.1069   \n",
      "1   hsa.letter.1.xml   L.1-2  https://gams.uni-graz.at/o:hsa.persons#P.1069   \n",
      "2  hsa.letter.10.xml  L.10-9  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "3  hsa.letter.10.xml  L.10-8  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "4  hsa.letter.10.xml  L.10-7  https://gams.uni-graz.at/o:hsa.persons#P.2100   \n",
      "\n",
      "              sender                                   receiver_id  \\\n",
      "0            Baissac  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "1            Baissac  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "2  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "3  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "4  Machado y Álvarez  https://gams.uni-graz.at/o:hsa.persons#P.109   \n",
      "\n",
      "     receiver        date                                               text  \\\n",
      "0  Schuchardt  1885-01-20  Ma Doudou vous envoie une petite brochure jaun...   \n",
      "1  Schuchardt  1885-01-20  Nous sommes anxieux l’un et l’autre d’avoir de...   \n",
      "2  Schuchardt        1882  No ando bien de salud, ni de dinero, ni de gus...   \n",
      "3  Schuchardt        1882  7º He publicado en el Giornale di filologia ro...   \n",
      "4  Schuchardt        1882  6º Coelho, Sébillot y Z. Consiglieri-Pedroso h...   \n",
      "\n",
      "  language                       keywords  word_count  \n",
      "0       fr                           None          17  \n",
      "1       fr                           None          31  \n",
      "2       es  Giornale di Filologia Romanza          57  \n",
      "3       es  Giornale di Filologia Romanza          44  \n",
      "4       es  Giornale di Filologia Romanza          21  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows, df = extract_paragraph_metadata_from_files(file_list)\n",
    "df[\"word_count\"] = df[\"text\"].apply(count_words)\n",
    "\n",
    "df_final = replace_long_paragraphs_with_sections(df, chunk_size=511, threshold=511)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336aafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'ca', 'cy', 'da', 'de', 'en', 'es', 'eu', 'fr', 'ft', 'hu', 'idb', 'io', 'it', 'la', 'lad', 'ms', 'nl', 'pap', 'pt', 'ro', 'roa']\n",
      "Empty DataFrame\n",
      "Columns: [pid, text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "languages = get_unique_languages(df)\n",
    "print(languages)\n",
    "\n",
    "df_511 = get_para_per_amount_of_words(df_final, amount = 511)\n",
    "print(df_511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5bc432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_by_pid(df):\n",
    "\n",
    "    def natural_key(pid):\n",
    "        return [int(s) if s.isdigit() else s.lower() \n",
    "                for s in re.split(r'(\\d+)', pid)]\n",
    "    \n",
    "    return df.sort_values(by=\"pid\", key=lambda col: col.map(natural_key)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f248c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver_id</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>keywords</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L.1-1</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1069</td>\n",
       "      <td>Baissac</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1885-01-20</td>\n",
       "      <td>Ma Doudou vous envoie une petite brochure jaun...</td>\n",
       "      <td>fr</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L.1-2</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1069</td>\n",
       "      <td>Baissac</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1885-01-20</td>\n",
       "      <td>Nous sommes anxieux l’un et l’autre d’avoir de...</td>\n",
       "      <td>fr</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L.3-1</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1068</td>\n",
       "      <td>Bähr</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1924-04-22</td>\n",
       "      <td>Bereits 5 Tage nach Abgang meines letzten Brie...</td>\n",
       "      <td>de</td>\n",
       "      <td>Revue de Linguistique Romane</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L.3-2-1</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1068</td>\n",
       "      <td>Bähr</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1924-04-22</td>\n",
       "      <td>Was die deutsche Verlagswelt angeht, so muss i...</td>\n",
       "      <td>de</td>\n",
       "      <td>Revue de Linguistique Romane</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L.3-2-2</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.1068</td>\n",
       "      <td>Bähr</td>\n",
       "      <td>https://gams.uni-graz.at/o:hsa.persons#P.109</td>\n",
       "      <td>Schuchardt</td>\n",
       "      <td>1924-04-22</td>\n",
       "      <td>sicherlich viel verdankt— erklärt, er wolle di...</td>\n",
       "      <td>de</td>\n",
       "      <td>Revue de Linguistique Romane</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pid                                      sender_id   sender  \\\n",
       "0    L.1-1  https://gams.uni-graz.at/o:hsa.persons#P.1069  Baissac   \n",
       "1    L.1-2  https://gams.uni-graz.at/o:hsa.persons#P.1069  Baissac   \n",
       "2    L.3-1  https://gams.uni-graz.at/o:hsa.persons#P.1068     Bähr   \n",
       "3  L.3-2-1  https://gams.uni-graz.at/o:hsa.persons#P.1068     Bähr   \n",
       "4  L.3-2-2  https://gams.uni-graz.at/o:hsa.persons#P.1068     Bähr   \n",
       "\n",
       "                                    receiver_id    receiver        date  \\\n",
       "0  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1885-01-20   \n",
       "1  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1885-01-20   \n",
       "2  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1924-04-22   \n",
       "3  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1924-04-22   \n",
       "4  https://gams.uni-graz.at/o:hsa.persons#P.109  Schuchardt  1924-04-22   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Ma Doudou vous envoie une petite brochure jaun...       fr   \n",
       "1  Nous sommes anxieux l’un et l’autre d’avoir de...       fr   \n",
       "2  Bereits 5 Tage nach Abgang meines letzten Brie...       de   \n",
       "3  Was die deutsche Verlagswelt angeht, so muss i...       de   \n",
       "4  sicherlich viel verdankt— erklärt, er wolle di...       de   \n",
       "\n",
       "                       keywords  word_count  \n",
       "0                          None          17  \n",
       "1                          None          31  \n",
       "2  Revue de Linguistique Romane         124  \n",
       "3  Revue de Linguistique Romane         511  \n",
       "4  Revue de Linguistique Romane          46  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = {\"source_file\"}\n",
    "df_final = df_final.drop(columns=drop)\n",
    "df_final = sort_by_pid(df_final)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b4b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dupes = df[df.duplicated(\"pid\")][\"pid\"].unique()\n",
    "print(dupes)\n",
    "print(len(dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e1b5a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [source_file, pid, sender_id, sender, receiver_id, receiver, date, text, language, keywords, word_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "empty_text_rows = df[df[\"text\"].isna() | (df[\"text\"].str.strip() == \"\")]\n",
    "print(empty_text_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335c3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"letters_py.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd03d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr True\n",
      "de True\n",
      "es True\n",
      "eu True\n",
      "da True\n",
      "pt True\n",
      "it True\n",
      "nl True\n",
      "ft False\n",
      "en True\n",
      "ca True\n",
      "la True\n",
      "ro True\n",
      "idb False\n",
      "cy False\n",
      "pap False\n",
      "io False\n",
      "hu True\n",
      "ar True\n",
      "lad False\n",
      "ms True\n",
      "roa False\n"
     ]
    }
   ],
   "source": [
    "langs = df_final['language'].unique()\n",
    "for lang in langs:\n",
    "    print(lang, stopwords.has_lang(lang))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tei-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
